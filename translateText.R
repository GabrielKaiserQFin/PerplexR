#' LLM: Translate Text
#'
#' @param text The text to be translated by LLM. If not provided, it will use what's copied on the clipboard.
#' @param toLanguage The language to be translated into.
#' @param PERPLEXITY_API_KEY PERPLEXITY API key.
#' @param modelSelection model choice. Default is mistral-7b-instruct.
#' @param systemRole Role for model. Default is: "You are a helpful assistant."
#' @param maxTokens The maximum integer of completion tokens returned by the API. The total number of tokens requested in max_tokens plus the number of prompt tokens sent in messages must not exceed the context window token limit of model requested. If left unspecified, then the model will generate tokens until either it reaches its stop token or the end of its context window
#' @param temperatur The amount of randomness in the response, valued between 0 inclusive and 2 exclusive. Higher values are more random, and lower values are more deterministic. You should either set temperature or top_p, but not both.
#' @param top_p The nucleus sampling threshold, valued between 0 and 1 inclusive. For each subsequent token, the model considers the results of the tokens with top_p probability mass. You should either alter temperature or top_p, but not both.
#' @param top_k The number of tokens to keep for highest top-k filtering, specified as an integer between 0 and 2048 inclusive. If set to 0, top-k filtering is disabled.
#' @param presence_penalty A value between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. Incompatible with frequency_penalty.
#' @param frequency_penalty A multiplicative penalty greater than 0. Values greater than 1.0 penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. A value of 1.0 means no penalty. Incompatible with presence_penalty.
#' @param proxy Default value is NULL. To execute Perplexity queries via a proxy server, specify the proxy address and port as an argument to the function instance.

#' @param returnType Default is 1, which cats the output, type 2 is unchanged and type 3 returns the output to the clipboard and returns TRUE
#'
#' @examples
#' \dontrun{
#' translateText("Dear Recipient, I hope this message finds you well.")
#' }
#'
#' @importFrom clipr read_clip
#'
#' @return A character value with the response generated by LLM.
#'
#' @export
#'
translateText <- function(text = clipr::read_clip(allow_non_interactive = TRUE), toLanguage = 'German',
                        PERPLEXITY_API_KEY = Sys.getenv("PERPLEXITY_API_KEY"),
                        modelSelection = c(
                            "codellama-34b-instruct",
                            "llama-2-70b-chat",
                            "mistral-7b-instruct",
                            "mixtral-8x7b-instruct",
                            "pplx-7b-chat",
                            "pplx-70b-chat",
                            "pplx-7b-online",
                            "pplx-70b-online"
                        ),
                        systemRole = "You are a helpful assistant.",
                        maxTokens = 265,
                        temperature = 1,
                        top_p = NULL,
                        top_k = 100,
                        presence_penalty = 0,
                        frequency_penalty = NULL,
                        proxy = NULL, returnType = 1) {

  # Replace all double strings with single string
  code <- gsub('"', "'", text)
  # Collapse the modified 'text' into a character vector
  code <- paste(text, collapse = "\n")
  # Create a prompt string by concatenating the input code
  prompt <- paste0('Can you translate the following text to ', toLanguage, ': "', text, '"')
  # Make an API request to Perplexity.AI using API_Request() function, passing the 'prompt' and other necessary arguments
  chatResponse <- API_Request(prompt, PERPLEXITY_API_KEY, modelSelection[1], systemRole, maxTokens, temperature, top_p, top_k, presence_penalty, frequency_penalty, proxy)
  # Parse the response using 'responseParser' and store the result into the 'chatResponse' variable
  chatResponse <- responseParser(chatResponse)

  # Return the chat response with the specified 'returnType'
  return(responseReturn(chatResponse, returnType))
}
